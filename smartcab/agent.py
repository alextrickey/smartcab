import random
import math
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator

class LearningAgent(Agent):
    """ An agent that learns to drive in the Smartcab world.
        This is the object you will be modifying. """ 

    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5):
        super(LearningAgent, self).__init__(env)     # Set the agent in the evironment 
        self.planner = RoutePlanner(self.env, self)  # Create a route planner
        self.valid_actions = self.env.valid_actions  # The set of valid actions

        # Set parameters of the learning agent
        self.learning = learning # Whether the agent is expected to learn
        self.Q = dict()          # Create a Q-table which will be a dictionary of tuples
        self.epsilon = epsilon   # Random exploration factor
        self.alpha = alpha       # Learning factor

        # Trial Number
        self.trial = 0
        
        # Epsilon Decay Function Parameters
        self.n = 100
        self.epsilon_val_at_n = 0.00285

    def reset(self, destination=None, testing=False):
        """ The reset function is called at the beginning of each trial.
            'testing' is set to True if testing trials are being used
            once training trials have completed. """ 

        # Select the destination as the new location to route to
        self.planner.route_to(destination)
        
        # Set testing learning rate and exploration factor
        if testing == True:
            self.alpha = 0
            self.epsilon = 0
        
        # Set training exploration factor
        else:
            # Linear
            if self.trial > 0: 
                self.epsilon = self.epsilon - self.epsilon_val_at_n
            
            # Update epsilon using a decay function of your choice
            self.trial += 1
            
            #Exponential (a^t equivalent to exp(-at) for this choice of a)
            #Best with n=200, epsilon_val_at_n=0.05, tolerance=0.001, alpha=0.04
            #a = math.log(self.epsilon_val_at_n)/self.n
            #self.epsilon = 0.4*math.exp(a*self.trial)
            
            print self.epsilon
            
            #Wavy Exponential (add a cosine term)
            #a = math.log(self.epsilon_val_at_n)/n
            #self.epsilon = math.exp(a*self.trial)
            #self.epsilon = self.epsilon*(1 + 0.4*(math.cos(math.pi/8*self.trial)))
            #self.epsilon = min(self.epsilon,1)
            #self.epsilon = max(self.epsilon,0)
            
            #Rational (Linear Denominator)
            #a = (1.0-self.epsilon_val_at_n)/((self.n-1.0)*self.epsilon_val_at_n)
            #b = 1.0-a
            #self.epsilon = 1/(a*self.trial + b)
                
            #Rational (Polynomial Denominator)
            #a = (1-self.epsilon_val_at_n)/((self.n**2-1)*self.epsilon_val_at_n)
            #b = 1-a
            #self.epsilon = 1/(a*(self.trial**2) + b)
        
            #Logistic
            #a = math.log(((self.epsilon_val_at_n)**2)
            #                /((1-self.epsilon_val_at_n)**2)) / (self.n-1.0)
            #b = math.log((1.0-self.epsilon_val_at_n)/self.epsilon_val_at_n) - a
            #self.epsilon = 1.0-1.0/(1.0+math.exp(a*self.trial+b))

            #if self.epsilon < 0 or self.epsilon > 1: 
            #    raise NameError('Epsilon out of bounds.')

        return None

    def createQ(self, state):
        """ The createQ function is called when a state is generated by the agent. """

        # When learning, check if the 'state' is not in the Q-table
        # If it is not, create a new dictionary for that state
        #   Then, for each action available, set the initial Q-value to 0.0
        #   For each action, set the Q-value for the state-action pair to 0
        # Updated initial value to 10 based upon this feedback in the forums:
        # https://discussions.udacity.com/t/in-which-function-do-i-update-the-q-table-and-how/204911/9
        if state not in self.Q.keys():
            action_names = [str(a) for a in self.valid_actions]
            self.Q[state] = {}
            for a in action_names:
                self.Q[state][a] = 10.0 
        
        return

    def build_state(self):
        """ The build_state function is called when the agent requests data from the 
            environment. The next waypoint, the intersection inputs, and the deadline 
            are all features available to the agent. """

        # Collect data about the environment
        waypoint = self.planner.next_waypoint() # The next waypoint 
        inputs = self.env.sense(self)           # Visual input - intersection light and traffic
        deadline = self.env.get_deadline(self)  # Remaining deadline


        # Set 'state' as a tuple of relevant data for the agent        
        state = (waypoint,
                inputs['light'],
                inputs['oncoming'],
                inputs['left'] != None)
        # This type of feature transformation was suggested by a Udacity Coach in the forum: 
        #https://discussions.udacity.com/t/i-dont-know-if-this-idea-is-a-kind-of-cheating/170894

        # When learning, check if the state is in the Q-table
        #   If it is not, create a dictionary in the Q-table for the current 'state'
        #   For each action, set the Q-value for the state-action pair to 0
        if self.learning == True: 
            self.createQ(state)
        
        return state


    def get_maxQ(self, state):
        """ The get_maxQ function is called when the agent is asked to find the
            maximum Q-value of all actions based on the 'state' the smartcab is in. """

        # Calculate the maximum Q-value of all actions for a given state
        best_action = 'TBD'
        for a in self.valid_actions:
            if best_action == 'TBD': 
                best_action = a
            elif self.Q[state][str(best_action)] < self.Q[state][str(a)]:
                best_action = a

        return best_action 
        
    def get_action_set(self, state):
        """ Return any untested actions for a given state, else return all 
            actions. """

        # Calculate the maximum Q-value of all actions for a given state
        action_set = []
        for a in self.valid_actions:
            if self.Q[state][str(a)] == 0:
                action_set.append(a)
        if len(action_set) == 0:
            action_set = self.valid_actions

        return action_set 


    def choose_action(self, state):
        """ The choose_action function is called when the agent is asked to choose
            which action to take, based on the 'state' the smartcab is in. """

        # Set the agent state and default action
        self.state = state
        self.next_waypoint = self.planner.next_waypoint()
        action = None
        
        # Choose an action
        if self.learning == False:
            # When not learning, choose a random action
            action = random.choice(self.valid_actions)
        else:
            # When learning, choose a random action with 'epsilon' probability
            #   Otherwise, choose an action with the highest Q-value for the current state
            #self.learning
            if random.random() > self.epsilon:
                action = self.get_maxQ(state)
            else:
                #returns set of unexplored actions (or all actions)
                action = random.choice(self.valid_actions)
                #action = random.choice(self.get_action_set(state))
 
        return action


    def learn(self, state, action, reward):
        """ The learn function is called after the agent completes an action and
            receives a reward. This function does not consider future rewards 
            when conducting learning. """

        # When learning, implement the value iteration update rule
        #   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')
        print state, action, self.Q[state]
        Qval = self.Q[state][str(action)] 
        self.Q[state][str(action)] = (1.0-self.alpha)*Qval + self.alpha*reward
        return


    def update(self):
        """ The update function is called when a time step is completed in the 
            environment for a given trial. This function will build the agent
            state, choose an action, receive a reward, and learn if enabled. """

        state = self.build_state()          # Get current state
        self.createQ(state)                 # Create 'state' in Q-table
        action = self.choose_action(state)  # Choose an action
        reward = self.env.act(self, action) # Receive a reward
        self.learn(state, action, reward)   # Q-learn

        return
        

def run():
    """ Driving function for running the simulation. 
        Press ESC to close the simulation, or [SPACE] to pause the simulation. """
    
    #Set random seed for displayed results
    random.seed(1901)
    
    ##############
    # Create the environment
    # Flags:
    #   verbose     - set to True to display additional output from the simulation
    #   num_dummies - discrete number of dummy agents in the environment, default is 100
    #   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)
    env = Environment()
    
    ##############
    # Create the driving agent
    # Flags:
    #   learning   - set to True to force the driving agent to use Q-learning
    #    * epsilon - continuous value for the exploration factor, default is 1
    #    * alpha   - continuous value for the learning rate, default is 0.5
    agent = env.create_agent(LearningAgent,learning=True,alpha=0.8)
    
    ##############
    # Follow the driving agent
    # Flags:
    #   enforce_deadline - set to True to enforce a deadline metric
    env.set_primary_agent(agent,enforce_deadline=True)

    ##############
    # Create the simulation
    # Flags:
    #   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds
    #   display      - set to False to disable the GUI if PyGame is enabled
    #   log_metrics  - set to True to log trial and simulation results to /logs
    #   optimized    - set to True to change the default log file name
    sim = Simulator(env,update_delay=0.001,display=False,log_metrics=True,optimized=True)

    ##############
    # Run the simulator
    # Flags:
    #   tolerance  - epsilon tolerance before beginning testing, default is 0.05 
    #   n_test     - discrete number of testing trials to perform, default is 0
    sim.run(n_test=100,tolerance=0.01)


if __name__ == '__main__':
    run()
